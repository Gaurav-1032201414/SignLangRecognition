# ğŸ¤Ÿ Sign Language Recognition Project

## ğŸ“ Introduction
This project aims to build a model that can accurately recognize sign language gestures from images or video feeds. The model currently just address three signs named, "Hello", "Thank you", "I love you", but can be trained for more by adding in the respective signs in the dataset.

## ğŸš€ Getting Started

### ğŸ“‹ Prerequisites
Firstly, clone the entire repository and set up the virtual enviornment as it give yu more flexibility in using python libraries. You can run the below command in the terminal to setup the virtual enviornment:

```
python -m venv .venv
```
To activate the enviorment, run the below command. Make sure you check if the scripts folder present in you envoirnment or else just run the above command again and you are good to go ahead.

for Windows
```
.venv\Scripts\activate  
```
for Mac or Linux
```
source .venv/bin/activate  
```
After this, install the requirements from the requirements.txt folder by running the below command. Note:- If you find in future that the txt file doesnot have certain libraries included, please append those and push the changes this repository and ping me to pull them.
```
pip install -r requirements.txt
```

## ğŸ“š Dataset
The dataset was creating using the mediapipe library. So inorder to create your own dataset, you have to go through the entire process of creating the dataset, or else ping me on email untawalegauarv20@gmail.com for the dataset and i will share it with you. The dataset only address 3 signs named, "Hello", "Thank You", and "I love You". 

Note: I would highly recommend you to create your own dataset as it will give you a better exxposer in understanding the mediapipe library. ğŸ§ ğŸ’¡
